Detector selected: <xfeatures2d_SURF 0x7f606dc990>

Descriptor selected: <xfeatures2d_FREAK 0x7f606dca30>

Training-set path: Datasets/FEI/Dataset/Train/

Test-set path: Datasets/FEI/Dataset/Test/

Number of Classes: 2

Extract Features

Processing the subdirectory named: happy 	[ 1 / 2 ]
Extracted FREAK 

Processing the subdirectory named: neutral 	[ 2 / 2 ]
Extracted FREAK 

Time: 00:00:01 

Create Bag of Visual Features

Training FREAK K-Means

Time: 00:00:00 

FREAK Training Data

Processing the subdirectory named: happy 	[ 1 / 2 ]

Processing the subdirectory named: neutral 	[ 2 / 2 ]

Time: 00:00:00 

FREAK Testing Data

Processing the subdirectory named: happy 	[ 1 / 2 ]

Processing the subdirectory named: neutral 	[ 2 / 2 ]

Time: 00:00:00 

Training FREAK MLP Models

FREAK MLP1

Training Set Evaluation

sgd + M = 0 + constant - train score: 0.50

FREAK MLP2

Training Set Evaluation

sgd + M = 0.3 + constant - train score: 0.50

FREAK MLP3

Training Set Evaluation

sgd + M = 0.6 + constant - train score: 0.50

FREAK MLP4

Training Set Evaluation

sgd + M = 0.9 + constant - train score: 0.50

FREAK MLP5

Training Set Evaluation

sgd + M = 0.9 + adaptive - train score: 0.50

FREAK MLP6

Training Set Evaluation

adam + M = 0.9 + constant - train score: 0.89

Testing FREAK MLP Model

FREAK MLP1

Testing Set Evaluation

sgd + M = 0 + constant - test score: 0.47

FREAK MLP2

Testing Set Evaluation

sgd + M = 0.3 + constant - test score: 0.47

FREAK MLP3

Testing Set Evaluation

sgd + M = 0.6 + constant - test score: 0.47

FREAK MLP4

Testing Set Evaluation

sgd + M = 0.9 + constant - test score: 0.51

FREAK MLP5

Testing Set Evaluation

sgd + M = 0.9 + adaptative - test score: 0.51

FREAK MLP6

Testing Set Evaluation

adam + M = 0.9 + constant - test score: 0.54

Classification Report

FREAK MLP1

Classification report for Classifier: 

MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(50, 30, 20, 20, 20, 30, 50),
       learning_rate='constant', learning_rate_init=0.01, max_iter=5000,
       momentum=0, nesterovs_momentum=True, power_t=0.5, random_state=2019,
       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,
       verbose=False, warm_start=False): 

              precision    recall  f1-score   support

      happy       0.00      0.00      0.00        59
    neutral       0.47      1.00      0.64        53

avg / total       0.22      0.47      0.30       112

FREAK MLP2

Classification report for Classifier: 

MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(50, 30, 20, 20, 20, 30, 50),
       learning_rate='constant', learning_rate_init=0.01, max_iter=5000,
       momentum=0.3, nesterovs_momentum=True, power_t=0.5,
       random_state=2019, shuffle=True, solver='sgd', tol=0.0001,
       validation_fraction=0.1, verbose=False, warm_start=False): 

              precision    recall  f1-score   support

      happy       0.00      0.00      0.00        59
    neutral       0.47      1.00      0.64        53

avg / total       0.22      0.47      0.30       112

FREAK MLP3

Classification report for Classifier: 

MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(50, 30, 20, 20, 20, 30, 50),
       learning_rate='constant', learning_rate_init=0.01, max_iter=5000,
       momentum=0.6, nesterovs_momentum=True, power_t=0.5,
       random_state=2019, shuffle=True, solver='sgd', tol=0.0001,
       validation_fraction=0.1, verbose=False, warm_start=False): 

              precision    recall  f1-score   support

      happy       0.00      0.00      0.00        59
    neutral       0.47      1.00      0.64        53

avg / total       0.22      0.47      0.30       112

FREAK MLP4

Classification report for Classifier: 

MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(50, 30, 20, 20, 20, 30, 50),
       learning_rate='constant', learning_rate_init=0.01, max_iter=5000,
       momentum=0.9, nesterovs_momentum=True, power_t=0.5,
       random_state=2019, shuffle=True, solver='sgd', tol=0.0001,
       validation_fraction=0.1, verbose=False, warm_start=False): 

              precision    recall  f1-score   support

      happy       0.52      0.97      0.67        59
    neutral       0.00      0.00      0.00        53

avg / total       0.27      0.51      0.36       112

FREAK MLP5

Classification report for Classifier: 

MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(50, 30, 20, 20, 20, 30, 50),
       learning_rate='adaptive', learning_rate_init=0.01, max_iter=5000,
       momentum=0.9, nesterovs_momentum=True, power_t=0.5,
       random_state=2019, shuffle=True, solver='sgd', tol=0.0001,
       validation_fraction=0.1, verbose=False, warm_start=False): 

              precision    recall  f1-score   support

      happy       0.52      0.97      0.67        59
    neutral       0.00      0.00      0.00        53

avg / total       0.27      0.51      0.36       112

FREAK MLP6

Classification report for Classifier: 

MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(50, 30, 20, 20, 20, 30, 50),
       learning_rate='constant', learning_rate_init=0.01, max_iter=5000,
       momentum=0.9, nesterovs_momentum=True, power_t=0.5,
       random_state=2019, shuffle=True, solver='adam', tol=0.0001,
       validation_fraction=0.1, verbose=False, warm_start=False): 

              precision    recall  f1-score   support

      happy       0.57      0.54      0.56        59
    neutral       0.52      0.55      0.53        53

avg / total       0.55      0.54      0.54       112

